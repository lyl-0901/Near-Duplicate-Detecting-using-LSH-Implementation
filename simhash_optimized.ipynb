{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30de5114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 21:18:22,408 [INFO] 日志系统已初始化，日志文件: D:\\spring2025\\UCUG2011-Discrete-Math\\Duplication_detecting\\processeddata_optimizedsimhash\\process.log\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "@dataclass() \n",
    "class ExperimentConfig:\n",
    "    # 核心参数\n",
    "    num_bands: int = 4\n",
    "    band_size: int = 16\n",
    "    threshold: float = 0.8\n",
    "    simhash_bits: int = 64\n",
    "    ngram_range: tuple = (1, 3)\n",
    "    chunk_size: int = 10000\n",
    "    \n",
    "    # 路径参数\n",
    "    raw_data_root: str = \"D:/spring2025/UCUG2011-Discrete-Math/project 1/discrete-math-project-1/src/data\"\n",
    "    processed_dir: str = \"D:/spring2025/UCUG2011-Discrete-Math/Duplication_detecting/processeddata_optimizedsimhash\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"参数校验\"\"\"\n",
    "        if self.num_bands * self.band_size != self.simhash_bits:\n",
    "            raise ValueError(\"num_bands * band_size must equal simhash_bits\")\n",
    "        if not (0 < self.threshold <= 1):\n",
    "            raise ValueError(\"Threshold must be in (0.0, 1.0]\")\n",
    "\n",
    "    @property\n",
    "    def param_hash(self) -> str:\n",
    "        \"\"\"包含所有参数的哈希标识\"\"\"\n",
    "        param_dict = {\n",
    "            \"num_bands\": self.num_bands,\n",
    "            \"band_size\": self.band_size,\n",
    "            \"threshold\": round(self.threshold, 2),\n",
    "            \"simhash_bits\": self.simhash_bits,\n",
    "            \"ngram_range\": self.ngram_range,\n",
    "            \"chunk_size\": self.chunk_size\n",
    "        }\n",
    "        return hashlib.md5(json.dumps(param_dict, sort_keys=True).encode()).hexdigest()[:10]\n",
    "        \n",
    "    @property\n",
    "    def params_dict(self) -> dict:\n",
    "        \"\"\"获取可序列化的参数字典\"\"\"\n",
    "        return {\n",
    "            'num_bands': self.num_bands,\n",
    "            'band_size': self.band_size,\n",
    "            'threshold': self.threshold,\n",
    "            'simhash_bits': self.simhash_bits,\n",
    "            'ngram_range': str(self.ngram_range),\n",
    "            'chunk_size': self.chunk_size\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def experiment_dir(self) -> Path:\n",
    "        \"\"\"动态实验目录\"\"\"\n",
    "        dir_path = Path(self.processed_dir) / f\"exp_{self.param_hash}\"\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        return dir_path\n",
    "    \n",
    "    preprocessed_path = Path(processed_dir) / \"preprocessed.csv\"\n",
    "    \n",
    "    @property\n",
    "    def signature_path(self) -> Path:\n",
    "        \"\"\"动态签名文件路径\"\"\"\n",
    "        return self.experiment_dir / f\"signatures_{self.param_hash}.parquet\"\n",
    "\n",
    "    @property\n",
    "    def candidates_path(self) -> Path:\n",
    "        \"\"\"动态候选对路径\"\"\"\n",
    "        return self.experiment_dir / f\"candidates_{self.param_hash}.csv\"\n",
    "    \n",
    "    @property\n",
    "    def performance_log_path(self) -> Path:\n",
    "        \"\"\"动态性能评估路径\"\"\"\n",
    "        return self.experiment_dir / f\"performance.json\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"参数校验并创建基础目录\"\"\"\n",
    "        if self.num_bands * self.band_size != self.simhash_bits:\n",
    "            raise ValueError(\"num_bands * band_size must equal simhash_bits\")\n",
    "        if not (0 < self.threshold <= 1):\n",
    "            raise ValueError(\"Threshold must be in (0.0, 1.0]\")\n",
    "        \n",
    "        # 新增：确保基础目录存在\n",
    "        Path(self.processed_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def configure_logging():\n",
    "    \"\"\"带自动目录创建的日志配置\"\"\"\n",
    "    log_dir = Path(ExperimentConfig.processed_dir)\n",
    "    log_file = log_dir / 'process.log'\n",
    "    \n",
    "    # 创建目录（如果不存在）\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 配置日志\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logging.info(f\"日志系统已初始化，日志文件: {log_file}\")\n",
    "\n",
    "# 调用配置函数\n",
    "configure_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0390d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_performance(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.monotonic()\n",
    "        process = psutil.Process()\n",
    "        start_mem = process.memory_info().rss\n",
    "        \n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        # 计算资源消耗\n",
    "        elapsed = time.monotonic() - start_time\n",
    "        end_mem = process.memory_info().rss\n",
    "        peak_mem = (end_mem - start_mem) // 1024 // 1024  # 转换为MB\n",
    "        \n",
    "        # 自动捕获配置对象\n",
    "        config = next((a for a in args if isinstance(a, ExperimentConfig)), None)\n",
    "        \n",
    "        if config:\n",
    "            # 记录到实验结果\n",
    "            performance_data = {\n",
    "                \"stage\": func.__name__,\n",
    "                \"time_sec\": round(elapsed, 2),\n",
    "                \"peak_memory_mb\": peak_mem,\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            \n",
    "            # 确保目录存在\n",
    "            config.experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # 合并历史日志（如果存在）\n",
    "            log_path = config.performance_log_path\n",
    "            existing_logs = []\n",
    "            if log_path.exists():\n",
    "                try:\n",
    "                    with open(log_path, \"r\") as f:\n",
    "                        existing_logs = json.load(f)\n",
    "                except json.JSONDecodeError:\n",
    "                    logging.warning(f\"性能日志 {log_path} 损坏，重置文件\")\n",
    "\n",
    "            # 追加新记录\n",
    "            existing_logs.append(performance_data)\n",
    "            \n",
    "            # 安全写入\n",
    "            try:\n",
    "                with open(log_path, \"w\") as f:\n",
    "                    json.dump(existing_logs, f, indent=2)\n",
    "            except IOError as e:\n",
    "                logging.error(f\"写入性能日志失败: {str(e)}\")\n",
    "\n",
    "        \n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "097b160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "def preprocess_document(text):\n",
    "    # 1. 删除结构化标记（包括_START_xxx和_NEWLINE_）\n",
    "    text = re.sub(\n",
    "        r'_START[＿_]\\w+\\b\\s*|_NEWLINE_',  # 同时匹配两种标记\n",
    "        '', \n",
    "        text,\n",
    "        flags=re.MULTILINE\n",
    "    )\n",
    "    \n",
    "    # 2. 标准化为小写\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 3. 去除标点符号（保留字母数字和空格）\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # 4. 合并连续空格并去除首尾空白\n",
    "    return ' '.join(text.strip().split())\n",
    "\n",
    "def preprocess_data(config: ExperimentConfig):\n",
    "    logging.info(\"Starting Step 1: Data Preprocessing\")\n",
    "    \n",
    "    # 加载原始数据\n",
    "    test_files = [str(p) for p in Path(f\"{config.raw_data_root}/test\").glob(\"*.arrow\")]\n",
    "    val_files = [str(p) for p in Path(f\"{config.raw_data_root}/validation\").glob(\"*.arrow\")]\n",
    "    \n",
    "    test_data = load_dataset(\"arrow\", data_files=test_files, split=\"train\")\n",
    "    val_data = load_dataset(\"arrow\", data_files=val_files, split=\"train\")\n",
    "    \n",
    "    # 转换为Pandas并重命名列\n",
    "    val_df = val_data.data.table.to_pandas().rename(columns={'wikidata_id': 'doc_id'})\n",
    "    test_df = test_data.data.table.to_pandas().rename(columns={'wikidata_id': 'doc_id'})\n",
    "    \n",
    "    # 添加source列\n",
    "    val_df['source'] = 'val'\n",
    "    test_df['source'] = 'test'\n",
    "    \n",
    "    # 合并数据集\n",
    "    merged_df = pd.concat([val_df[['doc_id', 'text', 'source']], \n",
    "                          test_df[['doc_id', 'text', 'source']]])\n",
    "    \n",
    "    # 分块处理文本（内存优化）\n",
    "    chunk_size = 10000\n",
    "    chunks = []\n",
    "    \n",
    "    with tqdm(total=len(merged_df), desc=\"Preprocessing Texts\") as pbar:\n",
    "        for i in range(0, len(merged_df), chunk_size):\n",
    "            chunk = merged_df.iloc[i:i+chunk_size].copy()\n",
    "            chunk['clean_text'] = chunk['text'].apply(\n",
    "                lambda x: preprocess_document(str(x))\n",
    "            )\n",
    "            # 删除原始文本列释放内存\n",
    "            chunk.drop(columns=['text'], inplace=True)\n",
    "            chunks.append(chunk)\n",
    "            pbar.update(len(chunk))\n",
    "    \n",
    "    processed_df = pd.concat(chunks)\n",
    "    \n",
    "    # 保存预处理结果\n",
    "    processed_df[['doc_id', 'source', 'clean_text']].to_csv(\n",
    "        ExperimentConfig.preprocessed_path,\n",
    "        index=False,\n",
    "        encoding='utf-8'\n",
    "    )\n",
    "    logging.info(f\"Saved preprocessed data to {ExperimentConfig.preprocessed_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93bc48a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class OptimizedTFIDFSimHasher:\n",
    "    def __init__(self, num_bits=64):\n",
    "        self.num_bits = num_bits\n",
    "        self._feature_cache = {}  # 特征哈希缓存\n",
    "        \n",
    "    def _get_feature_hash(self, feature_id: int) -> np.ndarray:\n",
    "        \"\"\"预计算特征哈希（使用稳定哈希）\"\"\"\n",
    "        if feature_id not in self._feature_cache:\n",
    "            # generate a 160-bit hash using SHA-1\n",
    "            hash_bytes = hashlib.sha256(str(feature_id).encode()).digest()\n",
    "            # convert to an integer and mask to num_bits bits\n",
    "            hash_int = int.from_bytes(hash_bytes, byteorder='big')\n",
    "            hash_int &= (1<<self.num_bits) - 1  # mask to num_bits bits\n",
    "            byte_length = (self.num_bits + 7) // 8  # compute byte length\n",
    "            hash_bytes = hash_int.to_bytes(byte_length, 'big')",
    "            # convert to binary representation using vectorized operations to avoid loops\n",
    "            bits = np.unpackbits(np.frombuffer(hash_bytes, dtype=np.uint8))\n",
    "            bits = bits[:self.num_bits]\n",
    "            self._feature_cache[feature_id] = np.where(bits, 1, -1).astype(np.int8)\n",
    "        return self._feature_cache[feature_id]\n",
    "\n",
    "    def batch_generate(self, tfidf_matrix):\n",
    "        num_docs= tfidf_matrix.shape[0]\n",
    "        hash_vectors=np.zeros((num_docs, self.num_bits), dtype=np.float32)\n",
    "\n",
    "        for doc_idx in range(num_docs):\n",
    "            row = tfidf_matrix[doc_idx]\n",
    "            indices = row.indices\n",
    "            data = row.data\n",
    "            feature_hashes = np.stack([self._get_feature_hash(i) for i in indices])\n",
    "            hash_vectors[doc_idx]=(feature_hashes*data[:, np.newaxis]).sum(axis=0)\n",
    "        \n",
    "        binary_sigs = np.where(hash_vectors > 0,'1','0')\n",
    "        return [int(''.join(row), 2) for row in binary_sigs]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d6a20ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import List, Union\n",
    "import random\n",
    "from pybloom_live import ScalableBloomFilter\n",
    "\n",
    "class SimHashLSHProcessor:\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        增强型LSH处理器，适配预处理流水线\n",
    "        :param num_bands: 分桶数（必须满足 num_bands * band_size == 64）\n",
    "        :param band_size: 每桶比特数\n",
    "        :param threshold: 相似度阈值 (0.0~1.0)\n",
    "        \"\"\"\n",
    "        self.config=config\n",
    "        self._validate_config()\n",
    "        self.num_bands = config.num_bands\n",
    "        self.band_size = config.band_size\n",
    "        self.threshold = config.threshold\n",
    "        self.simhash_bits=config.simhash_bits\n",
    "        self.inverted_index = defaultdict(set)\n",
    "        self.signatures = {}\n",
    "        self.doc_metadata = {}\n",
    "        self.seen_pairs = ScalableBloomFilter(\n",
    "        initial_capacity=1_000_000, \n",
    "        error_rate=1e-6\n",
    "        ) \n",
    "\n",
    "    def _validate_config(self):\n",
    "        \"\"\"参数校验\"\"\"\n",
    "        if self.config.num_bands * self.config.band_size != self.config.simhash_bits:\n",
    "            raise ValueError(\n",
    "                f\"num_bands({self.config.num_bands}) * band_size({self.config.band_size}) \"\n",
    "                f\"must equal to simhash_bits({self.config.simhash_bits})\"\n",
    "            )\n",
    "        if not (0 < self.config.threshold <= 1):\n",
    "            raise ValueError(f\"Threshold must be in (0.0, 1.0], got {self.config.threshold}\")\n",
    "\n",
    "    def load_signatures(self, parquet_path: Union[str, Path]):\n",
    "        \"\"\"\n",
    "        从Parquet文件加载预处理生成的签名数据\n",
    "        :param parquet_path: 预处理输出文件路径\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data = pd.read_parquet(parquet_path)\n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"签名文件不存在: {parquet_path}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logging.error(f\"加载Parquet失败: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        # 数据格式验证\n",
    "        required_fields = {'doc_id', 'source', 'simhash'}\n",
    "        if not required_fields.issubset(data.columns):\n",
    "            missing=required_fields - set(data.columns)\n",
    "            raise KeyError(f\"Missing required fields: {missing}\")\n",
    "        \n",
    "                # 加载到内存\n",
    "        for row in data.itertuples():\n",
    "            if len(row.simhash) != self.config.simhash_bits:\n",
    "                raise ValueError(\n",
    "                    f\"Length of the signature of the file {row.doc_id} is not equal to {self.config.simhash_bits}. \"\n",
    "                    f\"Expected: {self.config.simhash_bits}.\"\n",
    "                )\n",
    "                \n",
    "            self.signatures[row.doc_id] = row.simhash\n",
    "            self.doc_metadata[row.doc_id] = {'source': row.source}\n",
    "\n",
    "    def _band_signature(self, binary_str: str) -> List[str]:\n",
    "        \"\"\"优化分桶策略，适配预处理格式\"\"\"\n",
    "        return [\n",
    "            hashlib.sha1(binary_str[i*self.band_size : (i+1)*self.band_size].encode()).hexdigest()\n",
    "            for i in range(self.num_bands)\n",
    "        ]\n",
    "\n",
    "    def _hamming_similarity(self, sig1: str, sig2: str) -> float:\n",
    "        \"\"\"向量化汉明距离计算\"\"\"\n",
    "        arr1 = np.frombuffer(sig1.encode(), 'u1') - ord('0')\n",
    "        arr2 = np.frombuffer(sig2.encode(), 'u1') - ord('0')\n",
    "        return 1 - np.count_nonzero(arr1 != arr2) / self.simhash_bits\n",
    "    \n",
    "\n",
    "\n",
    "    def generate_candidates(self, output_path: str):\n",
    "        \"\"\"\n",
    "        生成候选对并保存为CSV\n",
    "        :param output_path: 输出文件路径\n",
    "        \"\"\"\n",
    "        seen_pairs = ScalableBloomFilter(\n",
    "            initial_capacity=1_000_000, \n",
    "            error_rate=1e-7\n",
    "        )\n",
    "        # 建立倒排索引\n",
    "        self.inverted_index.clear()\n",
    "        for doc_id, sig in tqdm(self.signatures.items(), desc=\"Building Inverted Index\"):\n",
    "            for band_hash in self._band_signature(sig):\n",
    "                self.inverted_index[band_hash].add(doc_id)\n",
    "\n",
    "\n",
    "        stats = {'total': 0, 'valid':0}\n",
    "        BUFFER_SIZE = 10_000  # 批量写入缓冲区大小\n",
    "        buffer = []\n",
    "        \n",
    "        with open(output_path, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"doc_id1\", \"source1\", \"doc_id2\", \"source2\", \"similarity\"])\n",
    "            \n",
    "            for doc_id1 in tqdm(self.signatures, desc=\"生成候选对\"):\n",
    "                candidate_ids = set()\n",
    "                for band_hash in self._band_signature(self.signatures[doc_id1]):\n",
    "                    candidate_ids.update(self.inverted_index.get(band_hash,set()))\n",
    "                \n",
    "                stats['total'] += len(candidate_ids)\n",
    "                \n",
    "                for doc_id2 in candidate_ids:\n",
    "                    if doc_id1 >= doc_id2:\n",
    "                        continue\n",
    "                    \n",
    "                    pair_key = f\"{doc_id1}_{doc_id2}\"\n",
    "                    if pair_key in seen_pairs:\n",
    "                        continue\n",
    "                    seen_pairs.add(pair_key)\n",
    "                    similarity = self._hamming_similarity(\n",
    "                        self.signatures[doc_id1], \n",
    "                        self.signatures[doc_id2]\n",
    "                    )\n",
    "                    \n",
    "                    if similarity >= self.config.threshold:\n",
    "                        seen_pairs.add(pair_key)\n",
    "                        stats['valid'] += 1\n",
    "                        \n",
    "                        source1 = self.doc_metadata[doc_id1]['source']\n",
    "                        source2 = self.doc_metadata[doc_id2]['source']\n",
    "                        writer.writerow([\n",
    "                            doc_id1, source1, \n",
    "                            doc_id2, source2, \n",
    "                            f\"{similarity:.4f}\"\n",
    "                        ])\n",
    "\n",
    "                        if buffer:\n",
    "                            writer.writerows\n",
    "        \n",
    "        logging.info(f\"生成候选对完成: 总候选 {stats['total']}, 有效 {stats['valid']}\")\n",
    "        return {\n",
    "            'total_candidates': stats['total'],\n",
    "            'valid_candidates': stats['valid'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bb96bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnionFind:\n",
    "    def __init__(self, elements):\n",
    "        self.parent = {e: e for e in elements}\n",
    "        self.rank = {e: 1 for e in elements}\n",
    "    \n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "    \n",
    "    def union(self, x, y):\n",
    "        root_x = self.find(x)\n",
    "        root_y = self.find(y)\n",
    "        \n",
    "        if root_x != root_y:\n",
    "            if self.rank[root_x] > self.rank[root_y]:\n",
    "                self.parent[root_y] = root_x\n",
    "            else:\n",
    "                self.parent[root_x] = root_y\n",
    "                if self.rank[root_x] == self.rank[root_y]:\n",
    "                    self.rank[root_y] += 1\n",
    "\n",
    "    def get_components(self):\n",
    "        components =defaultdict(list)\n",
    "        for elem in self.parent:\n",
    "            root = self.find(elem)\n",
    "            components[root].append(elem)\n",
    "        return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9eb24a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineIDF:\n",
    "    def __init__(self):\n",
    "        self.doc_count = 0\n",
    "        self.feature_df = np.zeros(2**20)  # 对应哈希空间\n",
    "        \n",
    "    def partial_fit(self, X):\n",
    "        # X是HashingVectorizer输出的计数矩阵\n",
    "        self.doc_count += X.shape[0]\n",
    "        # 统计每个特征出现的文档数\n",
    "        self.feature_df += (X > 0).sum(axis=0).A1  # 转换为ndarray\n",
    "    \n",
    "    def get_idf(self):\n",
    "        # 计算IDF值\n",
    "        idf = np.log((self.doc_count + 1) / (self.feature_df + 1)) + 1\n",
    "        return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81b46051",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentRunner:\n",
    "    def __init__(self, base_dir=ExperimentConfig.processed_dir):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.current_results = {}\n",
    "        self.summary_path = self.base_dir / \"experiment_summary.csv\" \n",
    "        self._init_summary_file()\n",
    "    \n",
    "    def _init_summary_file(self):\n",
    "        \"\"\"初始化空CSV文件\"\"\"\n",
    "        if not self.summary_path.exists():\n",
    "            columns = [\n",
    "                'experiment_id', 'timestamp',\n",
    "                *ExperimentConfig().params_dict.keys(),\n",
    "                'simhash_time_sec', 'lsh_time_sec',\n",
    "                'total_time_sec', 'peak_memory_mb',\n",
    "                'total_candidates', 'valid_candidates',\n",
    "                'total_duplicate_ratio', 'cross_set_ratio',\n",
    "                'intra_val_ratio', 'intra_test_ratio',\n",
    "            ]\n",
    "            pd.DataFrame(columns=columns).to_csv(self.summary_path, index=False)\n",
    "\n",
    "    def _update_summary(self, config: ExperimentConfig, metrics: dict):\n",
    "        if self.summary_path.exists():\n",
    "            existing_df = pd.read_csv(self.summary_path)\n",
    "        else:\n",
    "            existing_df = pd.DataFrame()\n",
    "\n",
    "        new_row = {\n",
    "            'experiment_id': config.param_hash,\n",
    "            'timestamp': pd.Timestamp.now().isoformat(),\n",
    "            **config.params_dict,\n",
    "            **metrics\n",
    "        }\n",
    "        updated_df = pd.concat([existing_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        updated_df.to_csv(self.summary_path, index=False)\n",
    "\n",
    "    def run_experiment(self, config: ExperimentConfig):\n",
    "        \"\"\"执行完整实验流程\"\"\"\n",
    "        self.current_results = {}\n",
    "        # exp_dir = self.base_dir / config.param_hash  \n",
    "        # exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 动态生成路径\n",
    "        logging.info(f\"签名文件路径: {config.signature_path}\")\n",
    "        logging.info(f\"候选对路径: {config.candidates_path}\")\n",
    "        \n",
    "        # 执行各阶段并记录性能\n",
    "        if config.preprocessed_path.exists():\n",
    "            logging.info(f\"使用现有预处理文件: {config.preprocessed_path}\")\n",
    "        else:\n",
    "            logging.info(f\"开始预处理数据: {config.preprocessed_path}\")\n",
    "            self._run_preprocessing(config)\n",
    "\n",
    "        simhash_start = time.time()\n",
    "        self._run_simhash(config)\n",
    "        simhash_time = time.time() - simhash_start\n",
    "        logging.info(f\"SimHash签名生成耗时: {simhash_time:.2f}秒\")\n",
    "        \n",
    "        lsh_start = time.time()\n",
    "        candidate_path=self._run_lsh(config)\n",
    "        lsh_time = time.time() - lsh_start\n",
    "\n",
    "        dup_metrics= self._calculate_duplication_metrics(candidate_path)\n",
    "        \n",
    "\n",
    "        try:\n",
    "            with open(config.performance_log_path) as f:\n",
    "                perf_data = json.load(f)\n",
    "                peak_memory = max(log['peak_memory_mb'] for log in perf_data)\n",
    "        except FileNotFoundError:\n",
    "            logging.error(\"性能日志未生成，请检查预处理阶段\")\n",
    "            peak_memory = -1  # 标记异常值\n",
    "        except Exception as e:\n",
    "            logging.error(f\"读取性能日志失败: {str(e)}\")\n",
    "            peak_memory = -1\n",
    "            \n",
    "        summary_metrics={\n",
    "            **config.params_dict,\n",
    "            'simhash_time_sec': round(simhash_time, 2),\n",
    "            'lsh_time_sec': round(lsh_time, 2),\n",
    "            'total_time_sec': round(simhash_time + lsh_time, 2),\n",
    "            'peak_memory_mb': peak_memory,\n",
    "            'total_candidates': self.current_results.get('total_candidates', 0),\n",
    "            'valid_candidates': self.current_results.get('valid_candidates', 0),\n",
    "            **dup_metrics\n",
    "        }\n",
    "        # 保存实验结果\n",
    "        self._update_summary(config, summary_metrics)\n",
    "        \n",
    "    def _run_preprocessing(self, config):\n",
    "        Path(config.processed_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        if config.preprocessed_path.exists():\n",
    "            logging.info(f\"使用现有预处理文件: {config.preprocessed_path}\")\n",
    "            return\n",
    "        \n",
    "        # 执行完整预处理\n",
    "        preprocess_data(config)\n",
    "    \n",
    "    @monitor_performance\n",
    "    def _run_simhash(self, config):\n",
    "        \"\"\"优化后的签名生成流程\"\"\"\n",
    "        # 加载预处理后的数据\n",
    "        preprocessed_path = Path(ExperimentConfig.preprocessed_path)\n",
    "        if not preprocessed_path.exists():\n",
    "            raise FileNotFoundError(f\"预处理文件不存在: {preprocessed_path}\")\n",
    "        \n",
    "        online_idf= OnlineIDF()\n",
    "        vectorizer = HashingVectorizer(\n",
    "        ngram_range=config.ngram_range,\n",
    "        stop_words='english',\n",
    "        n_features=2**20,  # 增加哈希空间避免冲突\n",
    "        alternate_sign=True, \n",
    "        binary=False\n",
    "        )\n",
    "        idf_reader=pd.read_csv(preprocessed_path, memory_map=True, usecols=['clean_text'],chunksize=config.chunk_size)\n",
    "        for chunk in idf_reader:\n",
    "            texts= chunk['clean_text'].astype('string').tolist()\n",
    "            X= vectorizer.transform(texts)\n",
    "            online_idf.partial_fit(X)\n",
    "\n",
    "        tfidf_trans= TfidfTransformer()\n",
    "        tfidf_trans.idf_= online_idf.get_idf()\n",
    "\n",
    "        dtype={'doc_id': 'string', 'source': 'category', 'clean_text': 'string'}\n",
    "        data_reader=pd.read_csv(preprocessed_path, usecols=['doc_id', 'source', 'clean_text'], \n",
    "                                dtype=dtype, chunksize=10000,memory_map=True)\n",
    "\n",
    "        signature_chunks=[]\n",
    "        for chunk in data_reader:\n",
    "            chunk=chunk.astype(dtype)\n",
    "            texts=chunk['clean_text'].tolist()\n",
    "            \n",
    "            X= vectorizer.fit_transform(texts)\n",
    "            X_tfidf = tfidf_trans.transform(X)\n",
    "\n",
    "            hasher= OptimizedTFIDFSimHasher(num_bits=config.simhash_bits)\n",
    "            signatures = hasher.batch_generate(X_tfidf)\n",
    "\n",
    "            chunk_df = pd.DataFrame({\n",
    "                'doc_id': chunk['doc_id'],\n",
    "                'source': chunk['source'],\n",
    "                'simhash': [format(sig, f'0{config.simhash_bits}b') for sig in signatures]\n",
    "            })\n",
    "            signature_chunks.append(chunk_df)\n",
    "            del chunk_df, X, X_tfidf#, signatures  # 释放内存\n",
    "\n",
    "        signature_df = pd.concat(signature_chunks, ignore_index=True) \n",
    "\n",
    "        sample = signature_df.sample(min(1000, len(signature_df)), random_state=42) \n",
    "        assert all(sample['simhash'].str.len() == config.simhash_bits), \"签名长度异常\"\n",
    "            \n",
    "        \n",
    "        # 保存签名文件\n",
    "        config.signature_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        signature_df.to_parquet(\n",
    "            config.signature_path,\n",
    "            engine='pyarrow',\n",
    "            compression='snappy',\n",
    "            index=False\n",
    "        )\n",
    "        logging.info(f\"SimHash签名已保存至: {config.signature_path}\")\n",
    "    \n",
    "    @monitor_performance\n",
    "    def _run_lsh(self, config):\n",
    "        '''generate candidates'''\n",
    "        processor = SimHashLSHProcessor(config=config)\n",
    "        try:\n",
    "            processor.load_signatures(config.signature_path)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"加载签名失败: {str(e)}\")\n",
    "            raise\n",
    "        result=processor.generate_candidates(config.candidates_path)\n",
    "        self.current_results.update({\n",
    "            'total_candidates': result['total_candidates'],\n",
    "            'valid_candidates': result['valid_candidates']\n",
    "        })\n",
    "\n",
    "        return config.candidates_path\n",
    "    \n",
    "    def _calculate_duplication_metrics(self, candidates_path: Path) -> dict:\n",
    "        \"\"\"计算详细的重复率指标\"\"\"\n",
    "\n",
    "        candidates = pd.read_csv(candidates_path)\n",
    "        preprocessed_path = Path(ExperimentConfig.preprocessed_path)\n",
    "        preprocessed = pd.read_csv(preprocessed_path)\n",
    "        total_docs = len(preprocessed)\n",
    "        \n",
    "        # 构建并查集\n",
    "        uf = UnionFind(preprocessed['doc_id'].tolist())\n",
    "        \n",
    "        # 合并所有候选对\n",
    "        for _, row in candidates.iterrows():\n",
    "            uf.union(row['doc_id1'], row['doc_id2'])\n",
    "        \n",
    "        # 统计各连通分量\n",
    "        components = uf.get_components()\n",
    "        \n",
    "        # 初始化统计指标\n",
    "        cross_set_duplicates = 0    # 跨数据集重复文档数\n",
    "        intra_val_duplicates = 0    # 验证集内部重复文档数\n",
    "        intra_test_duplicates = 0   # 测试集内部重复文档数\n",
    "        duplicate_docs =set()\n",
    "\n",
    "        # 分析每个连通分量\n",
    "        for component in components.values():\n",
    "            component_size = len(component)\n",
    "            if component_size < 2:\n",
    "                continue  # 忽略单文档组\n",
    "            \n",
    "            # 统计来源分布\n",
    "            sources = preprocessed[preprocessed['doc_id'].isin(component)]['source']\n",
    "            has_val = 'val' in sources.values\n",
    "            has_test = 'test' in sources.values\n",
    "            \n",
    "            # 统计重复文档数（去除基准计数）\n",
    "            duplicate_docs.update(component)\n",
    "            \n",
    "            if has_val and has_test:\n",
    "                cross_set_duplicates += component_size  # 整个分量都算作跨集重复\n",
    "            elif has_val:\n",
    "                intra_val_duplicates += component_size\n",
    "            elif has_test:\n",
    "                intra_test_duplicates += component_size\n",
    "        \n",
    "        # 计算重复率（基于总文档数）\n",
    "        return {\n",
    "            'total_docs': total_docs,\n",
    "            'total_duplicate_ratio': len(duplicate_docs) / total_docs if total_docs else 0,\n",
    "            'cross_set_ratio': cross_set_duplicates / total_docs if total_docs else 0,\n",
    "            'intra_val_ratio': intra_val_duplicates / total_docs if total_docs else 0,\n",
    "            'intra_test_ratio': intra_test_duplicates / total_docs if total_docs else 0\n",
    "\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7cffdd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总实验数: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Specific Experiments:   0%|          | 0/1 [00:00<?, ?it/s]2025-04-19 21:27:00,671 [INFO] 签名文件路径: D:\\spring2025\\UCUG2011-Discrete-Math\\Duplication_detecting\\processeddata_optimizedsimhash\\exp_dc3a27f2da\\signatures_dc3a27f2da.parquet\n",
      "2025-04-19 21:27:00,672 [INFO] 候选对路径: D:\\spring2025\\UCUG2011-Discrete-Math\\Duplication_detecting\\processeddata_optimizedsimhash\\exp_dc3a27f2da\\candidates_dc3a27f2da.csv\n",
      "2025-04-19 21:27:00,673 [INFO] 使用现有预处理文件: D:\\spring2025\\UCUG2011-Discrete-Math\\Duplication_detecting\\processeddata_optimizedsimhash\\preprocessed.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 实验 1/1 ===\n",
      "参数哈希: dc3a27f2da\n",
      "配置详情: {'num_bands': 4, 'band_size': 16, 'threshold': 0.8, 'simhash_bits': 64, 'ngram_range': (1, 3), 'chunk_size': 10000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 21:42:38,712 [INFO] SimHash签名已保存至: D:\\spring2025\\UCUG2011-Discrete-Math\\Duplication_detecting\\processeddata_optimizedsimhash\\exp_dc3a27f2da\\signatures_dc3a27f2da.parquet\n",
      "2025-04-19 21:42:38,841 [INFO] SimHash签名生成耗时: 938.17秒\n",
      "Building Inverted Index: 100%|██████████| 325871/325871 [00:03<00:00, 108190.62it/s]\n",
      "生成候选对: 100%|██████████| 325871/325871 [07:48<00:00, 696.20it/s]\n",
      "2025-04-19 21:50:30,964 [INFO] 生成候选对完成: 总候选 26528983, 有效 5530\n",
      "C:\\Users\\lyl61\\AppData\\Local\\Temp\\ipykernel_6432\\2593883538.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  updated_df = pd.concat([existing_df, pd.DataFrame([new_row])], ignore_index=True)\n",
      "Running Specific Experiments: 100%|██████████| 1/1 [24:13<00:00, 1453.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[性能摘要]\n",
      "总耗时: N/As\n",
      "内存峰值: N/AMB\n",
      "候选对总数: 26528983\n",
      "--------------------------------------------------\n",
      "已生成签名文件大小: 11449630 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    runner = ExperimentRunner()\n",
    "\n",
    "    # 手动定义参数组合 (已验证 num_bands * band_size = simhash_bits)\n",
    "    specified_params = [\n",
    "         {'num_bands': 4, 'band_size': 16, 'threshold': 0.8, 'simhash_bits': 64, 'ngram_range': (1,3), 'chunk_size': 10000},\n",
    "        # {'num_bands': 4, 'band_size': 16, 'threshold': 0.75, 'simhash_bits': 64, 'ngram_range': (1,3), 'chunk_size': 10000},\n",
    "        # {'num_bands': 4, 'band_size': 16, 'threshold': 0.7, 'simhash_bits': 64, 'ngram_range': (1,3), 'chunk_size': 10000},\n",
    "        # {'num_bands': 8, 'band_size': 8, 'threshold': 0.8, 'simhash_bits': 64, 'ngram_range': (1,3), 'chunk_size': 10000},\n",
    "        # {'num_bands': 8, 'band_size': 16, 'threshold': 0.8, 'simhash_bits': 128, 'ngram_range': (1,3), 'chunk_size': 10000},\n",
    "        # {'num_bands': 4, 'band_size': 32, 'threshold': 0.8, 'simhash_bits': 128, 'ngram_range': (1,3), 'chunk_size': 10000},\n",
    "        # {'num_bands': 4, 'band_size': 16, 'threshold': 0.8, 'simhash_bits': 64, 'ngram_range': (2,4), 'chunk_size': 10000},\n",
    "        # {'num_bands': 8, 'band_size': 16, 'threshold': 0.8, 'simhash_bits': 128, 'ngram_range': (2,4), 'chunk_size': 10000},\n",
    "        # {'num_bands': 4, 'band_size': 16, 'threshold': 0.8, 'simhash_bits': 64, 'ngram_range': (1,3), 'chunk_size': 50000},\n",
    "        # {'num_bands': 8, 'band_size': 16, 'threshold': 0.8, 'simhash_bits': 128, 'ngram_range': (1,3), 'chunk_size': 50000},\n",
    "        # {'num_bands': 4, 'band_size': 16, 'threshold': 0.8, 'simhash_bits': 64, 'ngram_range': (1,3), 'chunk_size': 100000},\n",
    "        # {'num_bands': 8, 'band_size': 16, 'threshold': 0.8, 'simhash_bits': 128, 'ngram_range': (1,3), 'chunk_size': 100000},\n",
    "    ]\n",
    "\n",
    "    print(f\"总实验数: {len(specified_params)}\")\n",
    "    \n",
    "    for i, params in enumerate(tqdm(specified_params, desc=\"Running Specific Experiments\")):\n",
    "            config = ExperimentConfig(\n",
    "                num_bands=params['num_bands'],\n",
    "                band_size=params['band_size'],\n",
    "                threshold=params['threshold'],\n",
    "                simhash_bits=params['simhash_bits'],\n",
    "                ngram_range=params['ngram_range'],\n",
    "                chunk_size=params['chunk_size']\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n=== 实验 {i+1}/{len(specified_params)} ===\")\n",
    "            print(f\"参数哈希: {config.param_hash}\")\n",
    "            print(f\"配置详情: {params}\")\n",
    "            \n",
    "            # 运行实验\n",
    "            runner.run_experiment(config)\n",
    "            \n",
    "            # 显示性能摘要\n",
    "            print(\"\\n[性能摘要]\")\n",
    "            print(f\"总耗时: {runner.current_results.get('total_time_sec', 'N/A')}s\")\n",
    "            print(f\"内存峰值: {runner.current_results.get('peak_memory_mb', 'N/A')}MB\")\n",
    "            print(f\"候选对总数: {runner.current_results.get('total_candidates', 'N/A')}\")\n",
    "            print(\"-\"*50)\n",
    "            \n",
    "        # except Exception as e:\n",
    "        #     print(f\"\\n!!! 实验 {i+1} 失败: {str(e)}\")\n",
    "        #     print(f\"失败参数: {params}\")\n",
    "            if config.signature_path.exists():\n",
    "                print(f\"已生成签名文件大小: {config.signature_path.stat().st_size} bytes\")\n",
    "            continue  # 继续执行后续实验\n",
    "\n",
    "\n",
    "\n",
    "def load_performance_log(log_path: Path) -> list:\n",
    "    \"\"\"安全加载性能日志\"\"\"\n",
    "    if not log_path.exists():\n",
    "        return []\n",
    "    try:\n",
    "        with open(log_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"警告: 性能日志 {log_path} 格式错误\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"加载日志失败: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e31adc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成带文本的候选对: 5530it [00:00, 11469.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "增强文件已生成：D:\\spring2025\\UCUG2011-Discrete-Math\\Duplication_detecting\\processeddata_optimizedsimhash\\exp_dc3a27f2da\\candidates_dc3a27f2da_text.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成带文本的候选对: 5530it [00:00, 11131.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "增强文件已生成：D:\\spring2025\\UCUG2011-Discrete-Math\\Duplication_detecting\\processeddata_optimizedsimhash\\exp_dc3a27f2da\\candidates_dc3a27f2da_text.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.55s/it]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_text_matched_candidates(config: ExperimentConfig):\n",
    "    \"\"\"为候选对生成包含文本内容的增强文件\"\"\"\n",
    "    # 加载预处理数据建立文本映射\n",
    "    text_lookup = pd.read_csv(config.preprocessed_path,\n",
    "                            usecols=['doc_id', 'clean_text']\n",
    "                            ).set_index('doc_id')['clean_text'].to_dict()\n",
    "\n",
    "    # 生成增强后的候选文件路径\n",
    "    enhanced_path = config.candidates_path.with_name(\n",
    "        config.candidates_path.name.replace(\".csv\", \"_text.csv\"))\n",
    "\n",
    "    # 分块处理候选对\n",
    "    chunks = pd.read_csv(config.candidates_path, chunksize=10000)\n",
    "    header = True\n",
    "\n",
    "    with tqdm(desc=\"生成带文本的候选对\") as pbar:\n",
    "        for chunk in chunks:\n",
    "            # 添加文本列\n",
    "            chunk['text1'] = chunk['doc_id1'].map(text_lookup)\n",
    "            chunk['text2'] = chunk['doc_id2'].map(text_lookup)\n",
    "\n",
    "            # 过滤无效匹配（约0.1%的数据）\n",
    "            valid_mask = chunk['text1'].notna() & chunk['text2'].notna()\n",
    "            chunk = chunk[valid_mask].copy()\n",
    "\n",
    "            # 保存增强数据\n",
    "            chunk[['doc_id1', 'doc_id2', 'text1', 'text2']].to_csv(\n",
    "                enhanced_path, \n",
    "                mode='w' if header else 'a',\n",
    "                header=header,\n",
    "                index=False\n",
    "            )\n",
    "            header = False\n",
    "            pbar.update(len(chunk))\n",
    "\n",
    "    print(f\"增强文件已生成：{enhanced_path}\")\n",
    "    return enhanced_path\n",
    "\n",
    "def batch_generate_text_matches(base_dir=ExperimentConfig.processed_dir):\n",
    "    \"\"\"批量处理所有实验的候选对\"\"\"\n",
    "    base_path = Path(base_dir)\n",
    "    summary_df = pd.read_csv(base_path / \"experiment_summary.csv\")\n",
    "\n",
    "    for _, row in tqdm(summary_df.iterrows(), total=len(summary_df)):\n",
    "        config = ExperimentConfig(\n",
    "            num_bands=row['num_bands'],\n",
    "            band_size=row['band_size'],\n",
    "            threshold=row['threshold'],\n",
    "            simhash_bits=row['simhash_bits'],\n",
    "            ngram_range=eval(row['ngram_range']),  # 将字符串转换为元组\n",
    "            chunk_size=row['chunk_size']\n",
    "        )\n",
    "\n",
    "        if config.candidates_path.exists():\n",
    "            generate_text_matched_candidates(config)\n",
    "        else:\n",
    "            print(f\"跳过未找到的候选文件：{config.candidates_path}\")\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 处理单个实验\n",
    "    config = ExperimentConfig()  # 使用默认参数\n",
    "    generate_text_matched_candidates(config)\n",
    "\n",
    "    # 批量处理所有实验\n",
    "    batch_generate_text_matches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd51d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "# from tqdm.auto import tqdm\n",
    "# import pandas as pd\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"主函数：执行多组参数实验\"\"\"\n",
    "#     # 初始化实验运行器\n",
    "#     runner = ExperimentRunner(base_dir=\"experiments\")\n",
    "    \n",
    "#     # 定义参数搜索空间\n",
    "#     param_grid = {\n",
    "#         'num_bands': [4, 8, 16],\n",
    "#         'band_size': [16, 8, 4],  # 需确保 num_bands * band_size = simhash_bits\n",
    "#         'threshold': [0.7, 0.75, 0.8],\n",
    "#         'simhash_bits': [64],     # 固定参数示例\n",
    "#         'ngram_range': [(1,3), (2,4)],\n",
    "#         'chunk_size': [10000, 50000]\n",
    "#     }\n",
    "\n",
    "#     # 生成有效参数组合\n",
    "    # valid_params = []\n",
    "    # for params in ParameterGrid(param_grid):\n",
    "    #     # 跳过无效组合（例如 num_bands * band_size != simhash_bits）\n",
    "    #     if params['num_bands'] * params['band_size'] != params['simhash_bits']:\n",
    "    #         continue\n",
    "    #     valid_params.append(params)\n",
    "    \n",
    "    # print(f\"总实验数: {len(valid_params)}\")\n",
    "    \n",
    "#     # 准备结果表格\n",
    "#     result_df = pd.DataFrame(columns=[\n",
    "#         'param_hash', 'num_bands', 'band_size', 'threshold',\n",
    "#         'simhash_bits', 'ngram_range', 'chunk_size',\n",
    "#         'total_candidates', 'valid_candidates', 'duplicate_rate',\n",
    "#         'time_cost', 'memory_peak'\n",
    "#     ])\n",
    "    \n",
    "#     # 执行实验\n",
    "#     for i, params in enumerate(tqdm(valid_params, desc=\"Running Experiments\")):\n",
    "#         try:\n",
    "#             # 创建配置对象\n",
    "#             config = ExperimentConfig(\n",
    "#                 num_bands=params['num_bands'],\n",
    "#                 band_size=params['band_size'],\n",
    "#                 threshold=params['threshold'],\n",
    "#                 simhash_bits=params['simhash_bits'],\n",
    "#                 ngram_range=params['ngram_range'],\n",
    "#                 chunk_size=params['chunk_size']\n",
    "#             )\n",
    "            \n",
    "#             # 运行实验\n",
    "#             start_time = time.time()\n",
    "#             runner.run_experiment(config)\n",
    "#             elapsed = time.time() - start_time\n",
    "            \n",
    "#             # 收集结果\n",
    "#             result_row = {\n",
    "#                 'param_hash': config.param_hash,\n",
    "#                 'num_bands': config.num_bands,\n",
    "#                 'band_size': config.band_size,\n",
    "#                 'threshold': config.threshold,\n",
    "#                 'simhash_bits': config.simhash_bits,\n",
    "#                 'ngram_range': str(config.ngram_range),  # 转换为字符串方便存储\n",
    "#                 'chunk_size': config.chunk_size,\n",
    "#                 'total_candidates': runner.current_results['total_candidates'],\n",
    "#                 'valid_candidates': runner.current_results['valid_candidates'],\n",
    "#                 'duplicate_rate': runner.current_results['duplicate_rate'],\n",
    "#                 'time_cost': elapsed,\n",
    "#                 'memory_peak': max([log['peak_memory_mb'] for log in config.performance_log])\n",
    "#             }\n",
    "            \n",
    "#             # 保存到表格\n",
    "#             result_df = pd.concat([result_df, pd.DataFrame([result_row])], ignore_index=True)\n",
    "            \n",
    "#             # 实时保存进度\n",
    "#             result_df.to_csv(\"experiments/summary.csv\", index=False)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"\\n参数组合 {params} 执行失败: {str(e)}\")\n",
    "#             continue\n",
    "    \n",
    "#     # 分析最佳参数\n",
    "#     print(\"\\n===== 实验结果分析 =====\")\n",
    "#     print(\"耗时最短配置:\")\n",
    "#     print(result_df.loc[result_df['time_cost'].idxmin()][['num_bands', 'band_size', 'time_cost']])\n",
    "    \n",
    "#     print(\"\\n重复率最高配置:\")\n",
    "#     print(result_df.loc[result_df['duplicate_rate'].idxmax()][['threshold', 'duplicate_rate']])\n",
    "    \n",
    "#     # 生成可视化报告\n",
    "#     generate_report(result_df)\n",
    "\n",
    "# def generate_report(df):\n",
    "#     \"\"\"生成可视化报告（示例）\"\"\"\n",
    "#     import matplotlib.pyplot as plt\n",
    "    \n",
    "#     # 耗时与分块大小关系\n",
    "#     plt.figure(figsize=(10,6))\n",
    "#     for ngram in df['ngram_range'].unique():\n",
    "#         subset = df[df['ngram_range'] == ngram]\n",
    "#         plt.scatter(subset['chunk_size'], subset['time_cost'], label=ngram)\n",
    "#     plt.xlabel('Chunk Size')\n",
    "#     plt.ylabel('Time Cost (s)')\n",
    "#     plt.legend()\n",
    "#     plt.savefig(\"experiments/time_vs_chunk.png\")\n",
    "    \n",
    "#     # 内存使用分布\n",
    "#     plt.figure(figsize=(10,6))\n",
    "#     df['memory_peak'].plot(kind='hist', bins=20)\n",
    "#     plt.xlabel('Peak Memory Usage (MB)')\n",
    "#     plt.savefig(\"experiments/memory_dist.png\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
