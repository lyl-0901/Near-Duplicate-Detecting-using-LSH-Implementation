{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30de5114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "\n",
    "@dataclass() \n",
    "class ExperimentConfig:\n",
    "    # 核心参数\n",
    "    num_bands: int = 4\n",
    "    band_size: int = 16\n",
    "    threshold: float = 0.8\n",
    "    simhash_bits: int = 64\n",
    "    ngram_range: tuple = (1, 3)\n",
    "    chunk_size: int = 10000\n",
    "    \n",
    "    # 路径参数\n",
    "    raw_data_root: str = \"D:/spring2025/UCUG2011-Discrete-Math/project 1/discrete-math-project-1/src/data\"\n",
    "    processed_dir: str = \"D:/spring2025/UCUG2011-Discrete-Math/project 1/discrete-math-project-1/src/processeddata\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"参数校验\"\"\"\n",
    "        if self.num_bands * self.band_size != self.simhash_bits:\n",
    "            raise ValueError(\"num_bands * band_size must equal simhash_bits\")\n",
    "        if not (0 < self.threshold <= 1):\n",
    "            raise ValueError(\"Threshold must be in (0.0, 1.0]\")\n",
    "\n",
    "    @property\n",
    "    def param_hash(self) -> str:\n",
    "        \"\"\"包含所有参数的哈希标识\"\"\"\n",
    "        param_dict = {\n",
    "            \"num_bands\": self.num_bands,\n",
    "            \"band_size\": self.band_size,\n",
    "            \"threshold\": round(self.threshold, 2),\n",
    "            \"simhash_bits\": self.simhash_bits,\n",
    "            \"ngram_range\": self.ngram_range,\n",
    "            \"chunk_size\": self.chunk_size\n",
    "        }\n",
    "        return hashlib.md5(json.dumps(param_dict, sort_keys=True).encode()).hexdigest()[:10]\n",
    "        \n",
    "    @property\n",
    "    def params_dict(self) -> dict:\n",
    "        \"\"\"获取可序列化的参数字典\"\"\"\n",
    "        return {\n",
    "            'num_bands': self.num_bands,\n",
    "            'band_size': self.band_size,\n",
    "            'threshold': self.threshold,\n",
    "            'simhash_bits': self.simhash_bits,\n",
    "            'ngram_range': str(self.ngram_range),\n",
    "            'chunk_size': self.chunk_size\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def experiment_dir(self) -> Path:\n",
    "        \"\"\"动态实验目录\"\"\"\n",
    "        dir_path = Path(self.processed_dir) / f\"exp_{self.param_hash}\"\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        return dir_path\n",
    "    \n",
    "    preprocessed_path = Path(processed_dir) / \"preprocessed.csv\"\n",
    "    \n",
    "    @property\n",
    "    def signature_path(self) -> Path:\n",
    "        \"\"\"动态签名文件路径\"\"\"\n",
    "        return self.experiment_dir / f\"signatures_{self.param_hash}.parquet\"\n",
    "\n",
    "    @property\n",
    "    def candidates_path(self) -> Path:\n",
    "        \"\"\"动态候选对路径\"\"\"\n",
    "        return self.experiment_dir / f\"candidates_{self.param_hash}.csv\"\n",
    "    \n",
    "    @property\n",
    "    def performance_log_path(self) -> Path:\n",
    "        \"\"\"动态性能评估路径\"\"\"\n",
    "        return self.experiment_dir / f\"performance.json\"\n",
    "    \n",
    "# 初始化日志\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(Path(ExperimentConfig.processed_dir)/'process.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0390d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_performance(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.monotonic()\n",
    "        process = psutil.Process()\n",
    "        start_mem = process.memory_info().rss\n",
    "        \n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        # 计算资源消耗\n",
    "        elapsed = time.monotonic() - start_time\n",
    "        end_mem = process.memory_info().rss\n",
    "        peak_mem = (end_mem - start_mem) // 1024 // 1024  # 转换为MB\n",
    "        \n",
    "        # 自动捕获配置对象\n",
    "        config = next((a for a in args if isinstance(a, ExperimentConfig)), None)\n",
    "        \n",
    "        if config:\n",
    "            # 记录到实验结果\n",
    "            performance_data = {\n",
    "                \"stage\": func.__name__,\n",
    "                \"time_sec\": round(elapsed, 2),\n",
    "                \"peak_memory_mb\": peak_mem,\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            \n",
    "            # 确保目录存在\n",
    "            config.experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # 合并历史日志（如果存在）\n",
    "            log_path = config.performance_log_path\n",
    "            existing_logs = []\n",
    "            if log_path.exists():\n",
    "                try:\n",
    "                    with open(log_path, \"r\") as f:\n",
    "                        existing_logs = json.load(f)\n",
    "                except json.JSONDecodeError:\n",
    "                    logging.warning(f\"性能日志 {log_path} 损坏，重置文件\")\n",
    "\n",
    "            # 追加新记录\n",
    "            existing_logs.append(performance_data)\n",
    "            \n",
    "            # 安全写入\n",
    "            try:\n",
    "                with open(log_path, \"w\") as f:\n",
    "                    json.dump(existing_logs, f, indent=2)\n",
    "            except IOError as e:\n",
    "                logging.error(f\"写入性能日志失败: {str(e)}\")\n",
    "\n",
    "        \n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "097b160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "def preprocess_document(text):\n",
    "    # 1. 仅删除结构化标记，保留后续内容\n",
    "    text = re.sub(\n",
    "        r'_START[＿_]\\w+\\b\\s*',  # 匹配_START_xxx及紧随的空白\n",
    "        '', \n",
    "        text,\n",
    "        flags=re.MULTILINE\n",
    "    )\n",
    "    \n",
    "    # 2. 标准化为小写\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 3. 去除标点符号（保留字母数字和空格）\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # 4. 合并连续空格并去除首尾空白\n",
    "    return ' '.join(text.strip().split())\n",
    "\n",
    "def preprocess_data(config: ExperimentConfig):\n",
    "    logging.info(\"Starting Step 1: Data Preprocessing\")\n",
    "    \n",
    "    # 加载原始数据\n",
    "    test_files = [str(p) for p in Path(f\"{config.raw_data_root}/test\").glob(\"*.arrow\")]\n",
    "    val_files = [str(p) for p in Path(f\"{config.raw_data_root}/validation\").glob(\"*.arrow\")]\n",
    "    \n",
    "    test_data = load_dataset(\"arrow\", data_files=test_files, split=\"train\")\n",
    "    val_data = load_dataset(\"arrow\", data_files=val_files, split=\"train\")\n",
    "    \n",
    "    # 转换为Pandas并重命名列\n",
    "    val_df = val_data.data.table.to_pandas().rename(columns={'wikidata_id': 'doc_id'})\n",
    "    test_df = test_data.data.table.to_pandas().rename(columns={'wikidata_id': 'doc_id'})\n",
    "    \n",
    "    # 添加source列\n",
    "    val_df['source'] = 'val'\n",
    "    test_df['source'] = 'test'\n",
    "    \n",
    "    # 合并数据集\n",
    "    merged_df = pd.concat([val_df[['doc_id', 'text', 'source']], \n",
    "                          test_df[['doc_id', 'text', 'source']]])\n",
    "    \n",
    "    # 分块处理文本（内存优化）\n",
    "    chunk_size = 10000\n",
    "    chunks = []\n",
    "    \n",
    "    with tqdm(total=len(merged_df), desc=\"Preprocessing Texts\") as pbar:\n",
    "        for i in range(0, len(merged_df), chunk_size):\n",
    "            chunk = merged_df.iloc[i:i+chunk_size].copy()\n",
    "            chunk['clean_text'] = chunk['text'].apply(\n",
    "                lambda x: preprocess_document(str(x))\n",
    "            )\n",
    "            # 删除原始文本列释放内存\n",
    "            chunk.drop(columns=['text'], inplace=True)\n",
    "            chunks.append(chunk)\n",
    "            pbar.update(len(chunk))\n",
    "    \n",
    "    processed_df = pd.concat(chunks)\n",
    "    \n",
    "    # 保存预处理结果\n",
    "    processed_df[['doc_id', 'source', 'clean_text']].to_csv(\n",
    "        ExperimentConfig.preprocessed_path,\n",
    "        index=False,\n",
    "        encoding='utf-8'\n",
    "    )\n",
    "    logging.info(f\"Saved preprocessed data to {ExperimentConfig.preprocessed_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93bc48a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class OptimizedTFIDFSimHasher:\n",
    "    def __init__(self, num_bits=64):\n",
    "        self.num_bits = num_bits\n",
    "        self._feature_cache = {}  # 特征哈希缓存\n",
    "        \n",
    "    def _get_feature_hash(self, feature_id: int) -> np.ndarray:\n",
    "        \"\"\"预计算特征哈希（使用稳定哈希）\"\"\"\n",
    "        if feature_id not in self._feature_cache:\n",
    "            # 使用SHA1确保哈希稳定性\n",
    "            hash_bytes = hashlib.sha1(str(feature_id).encode()).digest()\n",
    "            hash_int = int.from_bytes(hash_bytes, byteorder='big')\n",
    "            mask = (1 << self.num_bits) - 1\n",
    "            bits = [(hash_int & (1 << i)) >> i for i in range(self.num_bits)]\n",
    "            self._feature_cache[feature_id] = np.array(\n",
    "                [1 if b else -1 for b in bits], dtype=np.int8\n",
    "            )\n",
    "        return self._feature_cache[feature_id]\n",
    "\n",
    "    def generate_signature(self, tfidf_vector):\n",
    "        \"\"\"优化后的签名生成\"\"\"\n",
    "        hash_vector = np.zeros(self.num_bits, dtype=np.float32)\n",
    "        \n",
    "        # 只处理非零特征\n",
    "        indices = tfidf_vector.indices\n",
    "        data = tfidf_vector.data\n",
    "        \n",
    "        for i, val in zip(indices, data):\n",
    "            hash_vector += self._get_feature_hash(i) * val\n",
    "            \n",
    "        return int(''.join(['1' if x > 0 else '0' for x in hash_vector]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d6a20ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import List, Union\n",
    "\n",
    "class SimHashLSHProcessor:\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        增强型LSH处理器，适配预处理流水线\n",
    "        :param num_bands: 分桶数（必须满足 num_bands * band_size == 64）\n",
    "        :param band_size: 每桶比特数\n",
    "        :param threshold: 相似度阈值 (0.0~1.0)\n",
    "        \"\"\"\n",
    "        self.config=config\n",
    "        self._validate_config()\n",
    "        self.num_bands = config.num_bands\n",
    "        self.band_size = config.band_size\n",
    "        self.threshold = config.threshold\n",
    "        self.simhash_bits=config.simhash_bits\n",
    "\n",
    "        self.inverted_index = defaultdict(set)\n",
    "        self.signatures = {}\n",
    "        self.doc_metadata = {}\n",
    "\n",
    "    def _validate_config(self):\n",
    "        \"\"\"参数校验\"\"\"\n",
    "        if self.config.num_bands * self.config.band_size != self.config.simhash_bits:\n",
    "            raise ValueError(\n",
    "                f\"num_bands({self.config.num_bands}) * band_size({self.config.band_size}) \"\n",
    "                f\"must equal to simhash_bits({self.config.simhash_bits})\"\n",
    "            )\n",
    "        if not (0 < self.config.threshold <= 1):\n",
    "            raise ValueError(f\"Threshold must be in (0.0, 1.0], got {self.config.threshold}\")\n",
    "\n",
    "    def load_signatures(self, parquet_path: Union[str, Path]):\n",
    "        \"\"\"\n",
    "        从Parquet文件加载预处理生成的签名数据\n",
    "        :param parquet_path: 预处理输出文件路径\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data = pd.read_parquet(parquet_path)\n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"签名文件不存在: {parquet_path}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logging.error(f\"加载Parquet失败: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        \n",
    "        # 数据格式验证\n",
    "        required_fields = {'doc_id', 'source', 'simhash'}\n",
    "        if not required_fields.issubset(data.columns):\n",
    "            missing=required_fields - set(data.columns)\n",
    "            raise KeyError(f\"Missing required fields: {missing}\")\n",
    "        \n",
    "                # 加载到内存\n",
    "        for row in data.itertuples():\n",
    "            if len(row.simhash) != self.config.simhash_bits:\n",
    "                raise ValueError(\n",
    "                    f\"Length of the signature of the file {row.doc_id} is not equal to {self.config.simhash_bits}. \"\n",
    "                    f\"Expected: {self.config.simhash_bits}.\"\n",
    "                )\n",
    "                \n",
    "            self.signatures[row.doc_id] = row.simhash\n",
    "            self.doc_metadata[row.doc_id] = {'source': row.source}\n",
    "\n",
    "    def _band_signature(self, binary_str: str) -> List[str]:\n",
    "        \"\"\"优化分桶策略，适配预处理格式\"\"\"\n",
    "        return [\n",
    "            hashlib.sha1(binary_str[i*self.band_size : (i+1)*self.band_size].encode()).hexdigest()\n",
    "            for i in range(self.num_bands)\n",
    "        ]\n",
    "\n",
    "    def _hamming_similarity(self, sig1: str, sig2: str) -> float:\n",
    "        \"\"\"向量化汉明距离计算\"\"\"\n",
    "        arr1 = np.frombuffer(sig1.encode(), 'u1') - ord('0')\n",
    "        arr2 = np.frombuffer(sig2.encode(), 'u1') - ord('0')\n",
    "        return 1 - np.count_nonzero(arr1 != arr2) / self.simhash_bits\n",
    "\n",
    "    def generate_candidates(self, output_path: str):\n",
    "        \"\"\"\n",
    "        生成候选对并保存为CSV\n",
    "        :param output_path: 输出文件路径\n",
    "        \"\"\"\n",
    "        # 建立倒排索引\n",
    "        self.inverted_index.clear()\n",
    "        for doc_id, sig in tqdm(self.signatures.items(), desc=\"Building Inverted Index\"):\n",
    "            for band_hash in self._band_signature(sig):\n",
    "                self.inverted_index[band_hash].add(doc_id)\n",
    "\n",
    "        # 候选对生成\n",
    "        seen_pairs = set()\n",
    "        stats = {'total': 0, 'valid':0}\n",
    "        \n",
    "        with open(output_path, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"doc_id1\", \"source1\", \"doc_id2\", \"source2\", \"similarity\"])\n",
    "            \n",
    "            for doc_id1 in tqdm(self.signatures, desc=\"生成候选对\"):\n",
    "                candidate_ids = set()\n",
    "                for band_hash in self._band_signature(self.signatures[doc_id1]):\n",
    "                    candidate_ids.update(self.inverted_index[band_hash])\n",
    "                \n",
    "                stats['total'] += len(candidate_ids)\n",
    "                \n",
    "                for doc_id2 in candidate_ids:\n",
    "                    if doc_id1 >= doc_id2:\n",
    "                        continue\n",
    "                    \n",
    "                    pair_key = frozenset({doc_id1, doc_id2})\n",
    "                    if pair_key in seen_pairs:\n",
    "                        continue\n",
    "                        \n",
    "                    similarity = self._hamming_similarity(\n",
    "                        self.signatures[doc_id1], \n",
    "                        self.signatures[doc_id2]\n",
    "                    )\n",
    "                    \n",
    "                    if similarity >= self.config.threshold:\n",
    "                        seen_pairs.add(pair_key)\n",
    "                        stats['valid'] += 1\n",
    "                        \n",
    "                        source1 = self.doc_metadata[doc_id1]['source']\n",
    "                        source2 = self.doc_metadata[doc_id2]['source']\n",
    "                        writer.writerow([\n",
    "                            doc_id1, source1, \n",
    "                            doc_id2, source2, \n",
    "                            f\"{similarity:.4f}\"\n",
    "                        ])\n",
    "        \n",
    "        logging.info(f\"生成候选对完成: 总候选 {stats['total']}, 有效 {stats['valid']}\")\n",
    "        return {\n",
    "            'total_candidates': stats['total'],\n",
    "            'valid_candidates': stats['valid'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7bb96bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnionFind:\n",
    "    def __init__(self, elements):\n",
    "        self.parent = {e: e for e in elements}\n",
    "        self.rank = {e: 1 for e in elements}\n",
    "    \n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "    \n",
    "    def union(self, x, y):\n",
    "        root_x = self.find(x)\n",
    "        root_y = self.find(y)\n",
    "        \n",
    "        if root_x != root_y:\n",
    "            if self.rank[root_x] > self.rank[root_y]:\n",
    "                self.parent[root_y] = root_x\n",
    "            else:\n",
    "                self.parent[root_x] = root_y\n",
    "                if self.rank[root_x] == self.rank[root_y]:\n",
    "                    self.rank[root_y] += 1\n",
    "\n",
    "    def get_components(self):\n",
    "        components =defaultdict(list)\n",
    "        for elem in self.parent:\n",
    "            root = self.find(elem)\n",
    "            components[root].append(elem)\n",
    "        return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b46051",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentRunner:\n",
    "    def __init__(self, base_dir=ExperimentConfig.processed_dir):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.current_results = {}\n",
    "        self.summary_path = self.base_dir / \"experiment_summary.csv\" \n",
    "        self._init_summary_file()\n",
    "    \n",
    "    def _init_summary_file(self):\n",
    "        \"\"\"初始化空CSV文件\"\"\"\n",
    "        if not self.summary_path.exists():\n",
    "            columns = [\n",
    "                'experiment_id', 'timestamp',\n",
    "                *ExperimentConfig().params_dict.keys(),\n",
    "                'simhash_time_sec', 'lsh_time_sec',\n",
    "                'total_time_sec', 'peak_memory_mb',\n",
    "                'total_candidates', 'valid_candidates',\n",
    "                'total_duplicate_ratio', 'cross_set_ratio',\n",
    "                'intra_val_ratio', 'intra_test_ratio',\n",
    "            ]\n",
    "            pd.DataFrame(columns=columns).to_csv(self.summary_path, index=False)\n",
    "\n",
    "    def _update_summary(self, config: ExperimentConfig, metrics: dict):\n",
    "        if self.summary_path.exists():\n",
    "            existing_df = pd.read_csv(self.summary_path)\n",
    "        else:\n",
    "            existing_df = pd.DataFrame()\n",
    "\n",
    "        new_row = {\n",
    "            'experiment_id': config.param_hash,\n",
    "            'timestamp': pd.Timestamp.now().isoformat(),\n",
    "            **config.params_dict,\n",
    "            **metrics\n",
    "        }\n",
    "        updated_df = pd.concat([existing_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        updated_df.to_csv(self.summary_path, index=False)\n",
    "\n",
    "    def run_experiment(self, config: ExperimentConfig):\n",
    "        \"\"\"执行完整实验流程\"\"\"\n",
    "        self.current_results = {}\n",
    "        exp_dir = self.base_dir / config.param_hash  \n",
    "        exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 动态生成路径\n",
    "        logging.info(f\"签名文件路径: {config.signature_path}\")\n",
    "        logging.info(f\"候选对路径: {config.candidates_path}\")\n",
    "        \n",
    "        # 执行各阶段并记录性能\n",
    "        if config.preprocessed_path.exists():\n",
    "            logging.info(f\"使用现有预处理文件: {config.preprocessed_path}\")\n",
    "        else:\n",
    "            logging.info(f\"开始预处理数据: {config.preprocessed_path}\")\n",
    "            self._run_preprocessing(config)\n",
    "        simhash_start = time.time()\n",
    "        self._run_simhash(config)\n",
    "        simhash_time = time.time() - simhash_start\n",
    "        logging.info(f\"SimHash签名生成耗时: {simhash_time:.2f}秒\")\n",
    "        lsh_start = time.time()\n",
    "        candidate_path=self._run_lsh(config)\n",
    "        lsh_time = time.time() - lsh_start\n",
    "\n",
    "        dup_metrics= self._calculate_duplication_metrics(candidate_path)\n",
    "        \n",
    "\n",
    "        try:\n",
    "            with open(config.performance_log_path) as f:\n",
    "                perf_data = json.load(f)\n",
    "                peak_memory = max(log['peak_memory_mb'] for log in perf_data)\n",
    "        except FileNotFoundError:\n",
    "            logging.error(\"性能日志未生成，请检查预处理阶段\")\n",
    "            peak_memory = -1  # 标记异常值\n",
    "        except Exception as e:\n",
    "            logging.error(f\"读取性能日志失败: {str(e)}\")\n",
    "            peak_memory = -1\n",
    "            \n",
    "        summary_metrics={\n",
    "            **config.params_dict,\n",
    "            'simhash_time_sec': round(simhash_time, 2),\n",
    "            'lsh_time_sec': round(lsh_time, 2),\n",
    "            'total_time_sec': round(simhash_time + lsh_time, 2),\n",
    "            'peak_memory_mb': peak_memory,\n",
    "            **self.current_results\n",
    "            **dup_metrics\n",
    "        }\n",
    "        # 保存实验结果\n",
    "        self._update_summary(config, summary_metrics)\n",
    "        \n",
    "    def _run_preprocessing(self, config):\n",
    "        Path(config.processed_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        if config.preprocessed_path.exists():\n",
    "            logging.info(f\"使用现有预处理文件: {config.preprocessed_path}\")\n",
    "            return\n",
    "        \n",
    "        # 执行完整预处理\n",
    "        preprocess_data(config)\n",
    "    \n",
    "    @monitor_performance\n",
    "    def _run_simhash(self, config):\n",
    "        \"\"\"优化后的签名生成流程\"\"\"\n",
    "        # 加载预处理后的数据\n",
    "        preprocessed_path = Path(ExperimentConfig.preprocessed_path)\n",
    "        if not preprocessed_path.exists():\n",
    "            raise FileNotFoundError(f\"预处理文件不存在: {preprocessed_path}\")\n",
    "        preprocessed_data = pd.read_csv(preprocessed_path)\n",
    "        texts = preprocessed_data['clean_text'].tolist()\n",
    "        doc_ids = preprocessed_data['doc_id'].tolist()\n",
    "        sources = preprocessed_data['source'].tolist()\n",
    "        # 在 _run_simhash 方法中添加\n",
    "        assert len(doc_ids) == len(sources) == len(texts), \\\n",
    "            f\"数据长度不一致: doc_ids({len(doc_ids)}) vs sources({len(sources)}) vs texts({len(texts)})\"\n",
    "        \n",
    "        # 使用HashingVectorizer替代TfidfVectorizer\n",
    "        pipeline = make_pipeline(\n",
    "            HashingVectorizer(\n",
    "                ngram_range=config.ngram_range,\n",
    "                stop_words='english',\n",
    "                n_features=2**20  # 增加哈希空间避免冲突\n",
    "            ),\n",
    "            TfidfTransformer()\n",
    "        )\n",
    "        \n",
    "        # 分块处理\n",
    "        chunk_size = config.chunk_size\n",
    "        total_chunks = (len(texts)-1) // chunk_size + 1\n",
    "        signatures = []\n",
    "\n",
    "        with tqdm(total=total_chunks, desc=\"Generating SimHash Signatures\") as pbar:\n",
    "            for chunk_idx in range(0, len(texts), chunk_size):\n",
    "                chunk_texts = texts[chunk_idx:chunk_idx + chunk_size]\n",
    "                \n",
    "                # TF-IDF转换\n",
    "                tfidf_chunk = pipeline.fit_transform(chunk_texts)\n",
    "                \n",
    "                # 初始化SimHash生成器\n",
    "                hasher = OptimizedTFIDFSimHasher(num_bits=config.simhash_bits)\n",
    "                \n",
    "                # 并行生成签名\n",
    "                chunk_sigs = Parallel(n_jobs=-1, prefer=\"threads\")(\n",
    "                    delayed(hasher.generate_signature)(tfidf_chunk[j])\n",
    "                    for j in range(tfidf_chunk.shape[0])\n",
    "                )\n",
    "                \n",
    "                signatures.extend(chunk_sigs)\n",
    "                pbar.update(1)\n",
    "            \n",
    "            \n",
    "        # 构建签名DataFrame\n",
    "        signature_df = pd.DataFrame({\n",
    "            'doc_id': doc_ids,\n",
    "            'source': sources,\n",
    "            'simhash': [format(sig, f'0{config.simhash_bits}b') for sig in signatures]\n",
    "        })\n",
    "\n",
    "        # 验证签名长度\n",
    "        assert all(len(s) == config.simhash_bits \n",
    "            for s in signature_df['simhash']), \"签名长度异常\"\n",
    "        \n",
    "        # 保存签名文件\n",
    "        config.signature_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        signature_df.to_parquet(\n",
    "            config.signature_path,\n",
    "            engine='pyarrow',\n",
    "            compression='snappy'\n",
    "        )\n",
    "        logging.info(f\"SimHash签名已保存至: {config.signature_path}\")\n",
    "    \n",
    "    @monitor_performance\n",
    "    def _run_lsh(self, config):\n",
    "        '''generate candidates'''\n",
    "        processor = SimHashLSHProcessor(config=config)\n",
    "        try:\n",
    "            processor.load_signatures(config.signature_path)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"加载签名失败: {str(e)}\")\n",
    "            raise\n",
    "        result=processor.generate_candidates(config.candidates_path)\n",
    "        self.current_results.update({\n",
    "            'total_candidates': result['total_candidates'],\n",
    "            'valid_candidates': result['valid_candidates']\n",
    "        })\n",
    "\n",
    "        return config.candidates_path\n",
    "    \n",
    "    def _calculate_duplication_metrics(self, candidates_path: Path) -> dict:\n",
    "        \"\"\"计算详细的重复率指标\"\"\"\n",
    "\n",
    "        candidates = pd.read_csv(candidates_path)\n",
    "        preprocessed_path = Path(ExperimentConfig.preprocessed_path)\n",
    "        preprocessed = pd.read_csv(preprocessed_path)\n",
    "        total_docs = len(preprocessed)\n",
    "        \n",
    "        # 构建并查集\n",
    "        uf = UnionFind(preprocessed['doc_id'].tolist())\n",
    "        \n",
    "        # 合并所有候选对\n",
    "        for _, row in candidates.iterrows():\n",
    "            uf.union(row['doc_id1'], row['doc_id2'])\n",
    "        \n",
    "        # 统计各连通分量\n",
    "        components = uf.get_components()\n",
    "        \n",
    "        # 初始化统计指标\n",
    "        cross_set_duplicates = 0    # 跨数据集重复文档数\n",
    "        intra_val_duplicates = 0    # 验证集内部重复文档数\n",
    "        intra_test_duplicates = 0   # 测试集内部重复文档数\n",
    "        duplicate_docs =set()\n",
    "\n",
    "        # 分析每个连通分量\n",
    "        for component in components.values():\n",
    "            component_size = len(component)\n",
    "            if component_size < 2:\n",
    "                continue  # 忽略单文档组\n",
    "            \n",
    "            # 统计来源分布\n",
    "            sources = preprocessed[preprocessed['doc_id'].isin(component)]['source']\n",
    "            has_val = 'val' in sources.values\n",
    "            has_test = 'test' in sources.values\n",
    "            \n",
    "            # 统计重复文档数（去除基准计数）\n",
    "            duplicate_docs.update(component)\n",
    "            \n",
    "            if has_val and has_test:\n",
    "                cross_set_duplicates += component_size  # 整个分量都算作跨集重复\n",
    "            elif has_val:\n",
    "                intra_val_duplicates += component_size\n",
    "            elif has_test:\n",
    "                intra_test_duplicates += component_size\n",
    "        \n",
    "        # 计算重复率（基于总文档数）\n",
    "        return {\n",
    "            'total_docs': total_docs,\n",
    "            'total_duplicate_ratio': len(duplicate_docs) / total_docs if total_docs else 0,\n",
    "            'cross_set_ratio': cross_set_duplicates / total_docs if total_docs else 0,\n",
    "            'intra_val_ratio': intra_val_duplicates / total_docs if total_docs else 0,\n",
    "            'intra_test_ratio': intra_test_duplicates / total_docs if total_docs else 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7cffdd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总实验数: 36\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add143ddd74c46ddb342faa461fc3a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Experiments:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 14:56:10,862 [INFO] 签名文件路径: D:\\spring2025\\UCUG2011-Discrete-Math\\project 1\\discrete-math-project-1\\src\\processeddata\\exp_799da7c2a6\\signatures_799da7c2a6.parquet\n",
      "2025-04-18 14:56:10,863 [INFO] 候选对路径: D:\\spring2025\\UCUG2011-Discrete-Math\\project 1\\discrete-math-project-1\\src\\processeddata\\exp_799da7c2a6\\candidates_799da7c2a6.csv\n",
      "2025-04-18 14:56:10,864 [INFO] 开始预处理数据: D:\\spring2025\\UCUG2011-Discrete-Math\\project 1\\discrete-math-project-1\\src\\processeddata\\preprocessed.csv\n",
      "2025-04-18 14:56:10,866 [INFO] Starting Step 1: Data Preprocessing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 实验配置 ===\n",
      "参数哈希: 799da7c2a6\n",
      "实验目录: D:\\spring2025\\UCUG2011-Discrete-Math\\project 1\\discrete-math-project-1\\src\\processeddata\\exp_799da7c2a6\n",
      "预处理文件: D:\\spring2025\\UCUG2011-Discrete-Math\\project 1\\discrete-math-project-1\\src\\processeddata\\preprocessed.csv\n",
      "签名文件: D:\\spring2025\\UCUG2011-Discrete-Math\\project 1\\discrete-math-project-1\\src\\processeddata\\exp_799da7c2a6\\signatures_799da7c2a6.parquet\n",
      "候选对文件: D:\\spring2025\\UCUG2011-Discrete-Math\\project 1\\discrete-math-project-1\\src\\processeddata\\exp_799da7c2a6\\candidates_799da7c2a6.csv\n",
      "\n",
      "=== 开始执行 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b369caa607604a0383f88bb588591f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing Texts:   0%|          | 0/325871 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 14:57:00,996 [INFO] Saved preprocessed data to D:\\spring2025\\UCUG2011-Discrete-Math\\project 1\\discrete-math-project-1\\src\\processeddata\\preprocessed.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed43f1c8500a40ca9723b976c0b00044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating SimHash Signatures:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 88\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 88\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[48], line 53\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# 运行实验\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== 开始执行 ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 打印性能日志\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== 详细性能 ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[47], line 54\u001b[0m, in \u001b[0;36mExperimentRunner.run_experiment\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_preprocessing(config)\n\u001b[0;32m     53\u001b[0m simhash_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_simhash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m simhash_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m simhash_start\n\u001b[0;32m     56\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimHash签名生成耗时: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimhash_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m秒\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[42], line 7\u001b[0m, in \u001b[0;36mmonitor_performance.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      4\u001b[0m process \u001b[38;5;241m=\u001b[39m psutil\u001b[38;5;241m.\u001b[39mProcess()\n\u001b[0;32m      5\u001b[0m start_mem \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mmemory_info()\u001b[38;5;241m.\u001b[39mrss\n\u001b[1;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 计算资源消耗\u001b[39;00m\n\u001b[0;32m     10\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[47], line 139\u001b[0m, in \u001b[0;36mExperimentRunner._run_simhash\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    136\u001b[0m hasher \u001b[38;5;241m=\u001b[39m OptimizedTFIDFSimHasher(num_bits\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39msimhash_bits)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# 并行生成签名\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m chunk_sigs \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhasher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_signature\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf_chunk\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtfidf_chunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m signatures\u001b[38;5;241m.\u001b[39mextend(chunk_sigs)\n\u001b[0;32m    145\u001b[0m pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lyl61\\.conda\\envs\\jupyter\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lyl61\\.conda\\envs\\jupyter\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lyl61\\.conda\\envs\\jupyter\\lib\\site-packages\\joblib\\parallel.py:1768\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n\u001b[1;32m-> 1768\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   1769\u001b[0m     batched_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[0;32m   1771\u001b[0m \u001b[38;5;66;03m# Flatten the batched results to output one output at a time\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "def main():\n",
    "    \"\"\"单组参数测试主函数（适配新版配置）\"\"\"\n",
    "    # 初始化实验运行器\n",
    "    runner = ExperimentRunner()\n",
    "\n",
    "    # 定义参数搜索空间\n",
    "    param_grid = {\n",
    "        'num_bands': [4, 8, 16],\n",
    "        'band_size': [16, 8, 4],  # make sure num_bands * band_size = simhash_bits\n",
    "        'threshold': [0.7, 0.75, 0.8],\n",
    "        'simhash_bits': [64],\n",
    "        'ngram_range': [(1,3), (2,4)],\n",
    "        'chunk_size': [10000, 50000]\n",
    "    }\n",
    "    # param_grid = {\n",
    "    #     'num_bands': [8, 16, 4],\n",
    "    #     'band_size': [16, 8, 32],  # make sure num_bands * band_size = simhash_bits\n",
    "    #     'threshold': [0.7, 0.75, 0.8],\n",
    "    #     'simhash_bits': [128],\n",
    "    #     'ngram_range': [(1,3), (2,4)],\n",
    "    #     'chunk_size': [10000, 50000]\n",
    "    # }\n",
    "\n",
    "    valid_params = []\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        if params['num_bands'] * params['band_size'] != params['simhash_bits']:\n",
    "            continue\n",
    "        valid_params.append(params)\n",
    "    \n",
    "    print(f\"总实验数: {len(valid_params)}\")\n",
    "    \n",
    "    \n",
    "    for i, params in enumerate(tqdm(valid_params, desc=\"Running Experiments\")):\n",
    "        try:\n",
    "            # 创建配置对象\n",
    "            config = ExperimentConfig(\n",
    "                num_bands=params['num_bands'],\n",
    "                band_size=params['band_size'],\n",
    "                threshold=params['threshold'],\n",
    "                simhash_bits=params['simhash_bits'],\n",
    "                ngram_range=params['ngram_range'],\n",
    "                chunk_size=params['chunk_size']\n",
    "            )\n",
    "            print(\"=== 实验配置 ===\")\n",
    "            print(f\"参数哈希: {config.param_hash}\")\n",
    "            print(f\"实验目录: {config.experiment_dir}\")\n",
    "            print(f\"预处理文件: {config.preprocessed_path}\")\n",
    "            print(f\"签名文件: {config.signature_path}\")\n",
    "            print(f\"候选对文件: {config.candidates_path}\\n\")\n",
    "            # 运行实验\n",
    "            print(\"=== 开始执行 ===\")\n",
    "            runner.run_experiment(config)\n",
    "            # 打印性能日志\n",
    "            print(\"\\n=== 详细性能 ===\")\n",
    "            perf_log = load_performance_log(config.performance_log_path)\n",
    "            for log in perf_log:\n",
    "                print(f\"[{log['timestamp']}] {log['stage']}: \"\n",
    "                    f\"{log['time_sec']}s | 内存峰值: {log['peak_memory_mb']}MB\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n!!! 实验失败: {str(e)}\")\n",
    "            # 打印调试信息\n",
    "            print(\"\\n调试信息:\")\n",
    "            print(f\"当前目录内容: {list(config.experiment_dir.glob('*'))}\")\n",
    "            if config.performance_log_path.exists():\n",
    "                print(f\"性能日志大小: {config.performance_log_path.stat().st_size} bytes\")\n",
    "            raise\n",
    "            \n",
    "\n",
    "\n",
    "def load_performance_log(log_path: Path) -> list:\n",
    "    \"\"\"安全加载性能日志\"\"\"\n",
    "    if not log_path.exists():\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(log_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"警告: 性能日志 {log_path} 格式错误\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"加载日志失败: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd51d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "# from tqdm.auto import tqdm\n",
    "# import pandas as pd\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"主函数：执行多组参数实验\"\"\"\n",
    "#     # 初始化实验运行器\n",
    "#     runner = ExperimentRunner(base_dir=\"experiments\")\n",
    "    \n",
    "#     # 定义参数搜索空间\n",
    "#     param_grid = {\n",
    "#         'num_bands': [4, 8, 16],\n",
    "#         'band_size': [16, 8, 4],  # 需确保 num_bands * band_size = simhash_bits\n",
    "#         'threshold': [0.7, 0.75, 0.8],\n",
    "#         'simhash_bits': [64],     # 固定参数示例\n",
    "#         'ngram_range': [(1,3), (2,4)],\n",
    "#         'chunk_size': [10000, 50000]\n",
    "#     }\n",
    "\n",
    "#     # 生成有效参数组合\n",
    "    # valid_params = []\n",
    "    # for params in ParameterGrid(param_grid):\n",
    "    #     # 跳过无效组合（例如 num_bands * band_size != simhash_bits）\n",
    "    #     if params['num_bands'] * params['band_size'] != params['simhash_bits']:\n",
    "    #         continue\n",
    "    #     valid_params.append(params)\n",
    "    \n",
    "    # print(f\"总实验数: {len(valid_params)}\")\n",
    "    \n",
    "#     # 准备结果表格\n",
    "#     result_df = pd.DataFrame(columns=[\n",
    "#         'param_hash', 'num_bands', 'band_size', 'threshold',\n",
    "#         'simhash_bits', 'ngram_range', 'chunk_size',\n",
    "#         'total_candidates', 'valid_candidates', 'duplicate_rate',\n",
    "#         'time_cost', 'memory_peak'\n",
    "#     ])\n",
    "    \n",
    "#     # 执行实验\n",
    "#     for i, params in enumerate(tqdm(valid_params, desc=\"Running Experiments\")):\n",
    "#         try:\n",
    "#             # 创建配置对象\n",
    "#             config = ExperimentConfig(\n",
    "#                 num_bands=params['num_bands'],\n",
    "#                 band_size=params['band_size'],\n",
    "#                 threshold=params['threshold'],\n",
    "#                 simhash_bits=params['simhash_bits'],\n",
    "#                 ngram_range=params['ngram_range'],\n",
    "#                 chunk_size=params['chunk_size']\n",
    "#             )\n",
    "            \n",
    "#             # 运行实验\n",
    "#             start_time = time.time()\n",
    "#             runner.run_experiment(config)\n",
    "#             elapsed = time.time() - start_time\n",
    "            \n",
    "#             # 收集结果\n",
    "#             result_row = {\n",
    "#                 'param_hash': config.param_hash,\n",
    "#                 'num_bands': config.num_bands,\n",
    "#                 'band_size': config.band_size,\n",
    "#                 'threshold': config.threshold,\n",
    "#                 'simhash_bits': config.simhash_bits,\n",
    "#                 'ngram_range': str(config.ngram_range),  # 转换为字符串方便存储\n",
    "#                 'chunk_size': config.chunk_size,\n",
    "#                 'total_candidates': runner.current_results['total_candidates'],\n",
    "#                 'valid_candidates': runner.current_results['valid_candidates'],\n",
    "#                 'duplicate_rate': runner.current_results['duplicate_rate'],\n",
    "#                 'time_cost': elapsed,\n",
    "#                 'memory_peak': max([log['peak_memory_mb'] for log in config.performance_log])\n",
    "#             }\n",
    "            \n",
    "#             # 保存到表格\n",
    "#             result_df = pd.concat([result_df, pd.DataFrame([result_row])], ignore_index=True)\n",
    "            \n",
    "#             # 实时保存进度\n",
    "#             result_df.to_csv(\"experiments/summary.csv\", index=False)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"\\n参数组合 {params} 执行失败: {str(e)}\")\n",
    "#             continue\n",
    "    \n",
    "#     # 分析最佳参数\n",
    "#     print(\"\\n===== 实验结果分析 =====\")\n",
    "#     print(\"耗时最短配置:\")\n",
    "#     print(result_df.loc[result_df['time_cost'].idxmin()][['num_bands', 'band_size', 'time_cost']])\n",
    "    \n",
    "#     print(\"\\n重复率最高配置:\")\n",
    "#     print(result_df.loc[result_df['duplicate_rate'].idxmax()][['threshold', 'duplicate_rate']])\n",
    "    \n",
    "#     # 生成可视化报告\n",
    "#     generate_report(result_df)\n",
    "\n",
    "# def generate_report(df):\n",
    "#     \"\"\"生成可视化报告（示例）\"\"\"\n",
    "#     import matplotlib.pyplot as plt\n",
    "    \n",
    "#     # 耗时与分块大小关系\n",
    "#     plt.figure(figsize=(10,6))\n",
    "#     for ngram in df['ngram_range'].unique():\n",
    "#         subset = df[df['ngram_range'] == ngram]\n",
    "#         plt.scatter(subset['chunk_size'], subset['time_cost'], label=ngram)\n",
    "#     plt.xlabel('Chunk Size')\n",
    "#     plt.ylabel('Time Cost (s)')\n",
    "#     plt.legend()\n",
    "#     plt.savefig(\"experiments/time_vs_chunk.png\")\n",
    "    \n",
    "#     # 内存使用分布\n",
    "#     plt.figure(figsize=(10,6))\n",
    "#     df['memory_peak'].plot(kind='hist', bins=20)\n",
    "#     plt.xlabel('Peak Memory Usage (MB)')\n",
    "#     plt.savefig(\"experiments/memory_dist.png\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
